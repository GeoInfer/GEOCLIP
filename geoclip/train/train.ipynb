{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_r2_image_count():\n",
    "    # Set up R2 session using boto3\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id=R2_ACCESS_KEY,\n",
    "        aws_secret_access_key=R2_SECRET_KEY,\n",
    "        endpoint_url=R2_ENDPOINT\n",
    "    )\n",
    "\n",
    "    prefix = \"datasets/gmaps/world_sampling/world/world/images/\"\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(\n",
    "        Bucket=R2_BUCKET,\n",
    "        Prefix=prefix,\n",
    "    )\n",
    "\n",
    "    image_count = 0\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            image_count += len(page[\"Contents\"])\n",
    "    return image_count\n",
    "\n",
    "# Example usage to get the length:\n",
    "prefix = \"datasets/gmaps/world_sampling/world/world/images/\"\n",
    "len_images = get_r2_image_count()\n",
    "print(f\"Number of images in {R2_BUCKET}/{prefix} = {len_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728ca4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Download gmaps_shard_00001.tar.gz to memory\n",
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=R2_ACCESS_KEY,\n",
    "    aws_secret_access_key=R2_SECRET_KEY,\n",
    "    endpoint_url=R2_ENDPOINT\n",
    ")\n",
    "\n",
    "prefix = \"datasets/gmaps/world_sampling/world/world/images/\"\n",
    "filename = \"gmaps_shard_00003.tar.gz\"\n",
    "key = prefix + filename\n",
    "\n",
    "# Download to memory (BytesIO)\n",
    "tar_gz_data = io.BytesIO()\n",
    "s3.download_fileobj(R2_BUCKET, key, tar_gz_data)\n",
    "tar_gz_data.seek(0)  # Reset pointer to beginning\n",
    "\n",
    "# List files inside tar.gz\n",
    "file_list = []\n",
    "with tarfile.open(fileobj=tar_gz_data, mode='r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        if member.isfile():\n",
    "            file_list.append(member.name)\n",
    "\n",
    "# Store file list in variable\n",
    "files_inside = file_list\n",
    "\n",
    "print(f\"Number of files inside {filename}: {len(files_inside)}\")\n",
    "print(f\"\\nFiles inside {filename}:\")\n",
    "for i, file_path in enumerate(files_inside, 1):\n",
    "    print(f\"{i}. {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10baa610-15c2-44de-ab43-40dacb033d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Reset tar_gz_data pointer to beginning\n",
    "tar_gz_data.seek(0)\n",
    "\n",
    "# Find and read the first JSON file (excluding __metadata__.json)\n",
    "first_json_data = None\n",
    "first_json_filename = None\n",
    "\n",
    "with tarfile.open(fileobj=tar_gz_data, mode='r:gz') as tar:\n",
    "    # Find first JSON file that is not __metadata__.json\n",
    "    for member in tar.getmembers():\n",
    "        if member.isfile() and member.name.endswith('.json') and member.name != '__metadata__.json':\n",
    "            first_json_filename = member.name\n",
    "            # Extract and read the JSON file\n",
    "            json_file = tar.extractfile(member)\n",
    "            first_json_data = json.load(json_file)\n",
    "            break\n",
    "\n",
    "if first_json_data:\n",
    "    print(f\"First JSON file: {first_json_filename}\")\n",
    "    print(f\"\\nJSON content:\")\n",
    "    print(json.dumps(first_json_data, indent=2))\n",
    "else:\n",
    "    print(\"No JSON file found (excluding __metadata__.json)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import io\n",
    "import tarfile\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import threading\n",
    "\n",
    "# Get list of all tar.gz files\n",
    "def get_tar_gz_list():\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id=R2_ACCESS_KEY,\n",
    "        aws_secret_access_key=R2_SECRET_KEY,\n",
    "        endpoint_url=R2_ENDPOINT\n",
    "    )\n",
    "    \n",
    "    prefix = \"datasets/gmaps/world_sampling/world/world/images/\"\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=R2_BUCKET, Prefix=prefix)\n",
    "    \n",
    "    tar_gz_files = []\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page[\"Contents\"]:\n",
    "                if obj['Key'].endswith('.tar.gz'):\n",
    "                    tar_gz_files.append(obj['Key'])\n",
    "    \n",
    "    return sorted(tar_gz_files)\n",
    "\n",
    "# Get all tar.gz files\n",
    "tar_gz_list = get_tar_gz_list()\n",
    "print(f\"Found {len(tar_gz_list)} tar.gz files\")\n",
    "\n",
    "class TarGZDataset(Dataset):\n",
    "    def __init__(self, tar_gz_keys: List[str], r2_bucket: str, r2_endpoint: str, \n",
    "                 r2_access_key: str, r2_secret_key: str):\n",
    "        self.tar_gz_keys = tar_gz_keys\n",
    "        self.r2_bucket = r2_bucket\n",
    "        self.r2_endpoint = r2_endpoint\n",
    "        self.r2_access_key = r2_access_key\n",
    "        self.r2_secret_key = r2_secret_key\n",
    "        \n",
    "        # Thread-local storage for worker-specific data\n",
    "        self.worker_data = threading.local()\n",
    "        \n",
    "        # Pre-compute total samples across all tar.gz files\n",
    "        # This is an approximation - actual count will be computed per worker\n",
    "        self._total_samples = None\n",
    "    \n",
    "    def _get_worker_tar_gz(self, worker_id: int) -> Optional[str]:\n",
    "        \"\"\"Get the tar.gz file assigned to this worker\"\"\"\n",
    "        if worker_id < len(self.tar_gz_keys):\n",
    "            return self.tar_gz_keys[worker_id]\n",
    "        return None\n",
    "    \n",
    "    def _load_tar_gz_data(self, tar_gz_key: str):\n",
    "        \"\"\"Load and cache tar.gz data for this worker\"\"\"\n",
    "        if not hasattr(self.worker_data, 'tar_gz_key') or self.worker_data.tar_gz_key != tar_gz_key:\n",
    "            # Get worker ID for logging\n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            worker_id = worker_info.id if worker_info else 0\n",
    "            print(f\"Worker {worker_id} loading tar.gz: {tar_gz_key}\")\n",
    "            \n",
    "            # Download tar.gz to memory\n",
    "            session = boto3.session.Session()\n",
    "            s3 = session.client(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id=self.r2_access_key,\n",
    "                aws_secret_access_key=self.r2_secret_key,\n",
    "                endpoint_url=self.r2_endpoint\n",
    "            )\n",
    "            \n",
    "            tar_gz_data = io.BytesIO()\n",
    "            s3.download_fileobj(self.r2_bucket, tar_gz_key, tar_gz_data)\n",
    "            tar_gz_data.seek(0)\n",
    "            \n",
    "            # Extract all JSON and image pairs\n",
    "            samples = []\n",
    "            with tarfile.open(fileobj=tar_gz_data, mode='r:gz') as tar:\n",
    "                # Build a mapping of JSON files to their data\n",
    "                json_data_map = {}\n",
    "                image_data_map = {}\n",
    "                \n",
    "                for member in tar.getmembers():\n",
    "                    if member.isfile():\n",
    "                        if member.name.endswith('.json') and member.name != '__metadata__.json':\n",
    "                            json_file = tar.extractfile(member)\n",
    "                            json_data = json.load(json_file)\n",
    "                            json_data_map[member.name] = json_data\n",
    "                        elif member.name.endswith('.jpg'):\n",
    "                            image_file = tar.extractfile(member)\n",
    "                            image_data = image_file.read()\n",
    "                            image_data_map[member.name] = image_data\n",
    "                \n",
    "                # Match JSON files with their corresponding images\n",
    "                for json_name, json_data in json_data_map.items():\n",
    "                    if 'lat' in json_data and 'lon' in json_data:\n",
    "                        # Find corresponding image (remove .json extension and add .jpg)\n",
    "                        image_name = json_name.replace('.json', '.jpg')\n",
    "                        if image_name in image_data_map:\n",
    "                            samples.append({\n",
    "                                'image': image_data_map[image_name],\n",
    "                                'lat': json_data['lat'],\n",
    "                                'lon': json_data['lon'],\n",
    "                                'json_data': json_data\n",
    "                            })\n",
    "            \n",
    "            print(f\"Worker {worker_id} loaded {len(samples)} samples from {tar_gz_key}\")\n",
    "            \n",
    "            # Cache in thread-local storage\n",
    "            self.worker_data.tar_gz_key = tar_gz_key\n",
    "            self.worker_data.samples = samples\n",
    "            self.worker_data.current_idx = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return 1 - each worker will return all samples from its tar.gz in one batch\n",
    "        return 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get worker ID\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            worker_id = 0\n",
    "        else:\n",
    "            worker_id = worker_info.id\n",
    "        \n",
    "        # Get tar.gz for this worker\n",
    "        tar_gz_key = self._get_worker_tar_gz(worker_id)\n",
    "        if tar_gz_key is None:\n",
    "            raise IndexError(f\"Worker {worker_id} has no assigned tar.gz\")\n",
    "        \n",
    "        # Load tar.gz data if not already loaded\n",
    "        self._load_tar_gz_data(tar_gz_key)\n",
    "        \n",
    "        # Get all samples from worker's cached data\n",
    "        if not hasattr(self.worker_data, 'samples') or len(self.worker_data.samples) == 0:\n",
    "            raise IndexError(f\"No samples in tar.gz {tar_gz_key}\")\n",
    "        \n",
    "        # Process all samples from this tar.gz\n",
    "        transform = transforms.ToTensor()\n",
    "        all_images = []\n",
    "        all_lats = []\n",
    "        all_lons = []\n",
    "        all_json_data = []\n",
    "        \n",
    "        for sample in self.worker_data.samples:\n",
    "            # Load image from bytes and convert to tensor\n",
    "            image = Image.open(io.BytesIO(sample['image'])).convert('RGB')\n",
    "            image_tensor = transform(image)\n",
    "            \n",
    "            all_images.append(image_tensor)\n",
    "            all_lats.append(torch.tensor(sample['lat'], dtype=torch.float32))\n",
    "            all_lons.append(torch.tensor(sample['lon'], dtype=torch.float32))\n",
    "            all_json_data.append(sample['json_data'])\n",
    "        \n",
    "        # Return all samples as a batch\n",
    "        return {\n",
    "            'image': all_images,  # List of tensors\n",
    "            'lat': all_lats,      # List of tensors\n",
    "            'lon': all_lons,      # List of tensors\n",
    "            'json_data': all_json_data  # List of dicts\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle dicts in json_data\"\"\"\n",
    "    from torch.utils.data._utils.collate import default_collate\n",
    "    \n",
    "    # Each item in batch is a dict with lists (all samples from one tar.gz)\n",
    "    # We need to flatten across all workers\n",
    "    all_images = []\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    all_json_data = []\n",
    "    \n",
    "    # Flatten the batch (each worker contributes all its samples)\n",
    "    for item in batch:\n",
    "        all_images.extend(item['image'])  # item['image'] is already a list\n",
    "        all_lats.extend(item['lat'])     # item['lat'] is already a list\n",
    "        all_lons.extend(item['lon'])     # item['lon'] is already a list\n",
    "        all_json_data.extend(item['json_data'])  # item['json_data'] is already a list\n",
    "    \n",
    "    # Collate all samples together\n",
    "    result = {\n",
    "        'image': default_collate(all_images),\n",
    "        'lat': default_collate(all_lats),\n",
    "        'lon': default_collate(all_lons),\n",
    "        'json_data': all_json_data  # Keep as list (can't be collated)\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initialize worker - each worker will process a different tar.gz\"\"\"\n",
    "    pass  # The Dataset handles worker-specific logic\n",
    "\n",
    "# Create dataset\n",
    "dataset = TarGZDataset(\n",
    "    tar_gz_keys=tar_gz_list,\n",
    "    r2_bucket=R2_BUCKET,\n",
    "    r2_endpoint=R2_ENDPOINT,\n",
    "    r2_access_key=R2_ACCESS_KEY,\n",
    "    r2_secret_key=R2_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create dataloader with multiple workers\n",
    "# Each worker will process a different tar.gz\n",
    "num_workers = min(4, len(tar_gz_list))  # Use up to 4 workers or number of tar.gz files\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Created DataLoader with {num_workers} workers\")\n",
    "print(f\"Each worker will process a different tar.gz file\")\n",
    "print(f\"Each batch contains all images from the tar.gz files processed by workers\")\n",
    "print(f\"\\nExample usage:\")\n",
    "print(f\"  for batch in dataloader:\")\n",
    "print(f\"      image = batch['image']      # Tensor [B, C, H, W] - all images from all workers\")\n",
    "print(f\"      lat = batch['lat']          # Tensor [B] - latitudes for all images\")\n",
    "print(f\"      lon = batch['lon']          # Tensor [B] - longitudes for all images\")\n",
    "print(f\"      json_data = batch['json_data']  # List of dicts (one per image)\")\n",
    "print(f\"      print(f'Batch size: {{image.shape[0]}}')  # Number of images in this batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1672d1-ce95-4009-8c0f-8d7cb3b43327",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch[\"image\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16944ca-ef1b-4241-992f-d77a95c319bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (georasters)",
   "language": "python",
   "name": "georasters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
